# Simple autogenerated Python bindings for ggml

This folder contains:
- Scripts to generate full Python bindings (and type stubs) from ggml's headers.
- Some core utils to ease up interop w/ Numpy
  - `ggml.utils.init` builds a context that's freed automatically when its pointer is GC'd
  - `ggml.utils.copy` can copy between numpy & ggml tensors of the same shape, with automatic (de)quantization
  - `ggml.utils.numpy` returns a numpy view over a ggml tensor; if it's quantized, it returns a copy (requires `allow_copy=True`)
- Some basic examples... including a port of [llama2.c](https://github.com/karpathy/llama2.c) (WIP)

It makes it trivial to integrate ggml in a Python ecosystem.

```python
from ggml import lib, ffi
from ggml.utils import init, copy, numpy
import numpy as np

ctx = init(mem_size=12*1024*1024)
n = 256
n_threads = 4

a = lib.ggml_new_tensor_1d(ctx, lib.GGML_TYPE_Q5_K, n)
b = lib.ggml_new_tensor_1d(ctx, lib.GGML_TYPE_F32, n) # Can't both be quantized
sum = lib.ggml_add(ctx, a, b) # all zeroes for now. Will be quantized too!

gf = ffi.new('struct ggml_cgraph*')
lib.ggml_build_forward_expand(gf, sum)

copy(np.array([i for i in range(n)], np.float32), a)
copy(np.array([i*100 for i in range(n)], np.float32), b)

lib.ggml_graph_compute_with_ctx(ctx, gf, n_threads)

print(numpy(a, allow_copy=True))
#  0.    1.0439453   2.0878906   3.131836    4.1757812   5.2197266. ...
print(numpy(b))
#  0.  100.        200.        300.        400.        500.         ...
print(numpy(sum, allow_copy=True))
#  0.  105.4375    210.875     316.3125    421.75      527.1875     ...
```

### Prerequisites

Some Python deps, and it's better if you have a llama.cpp repo lying around (but not required, we can build against either repos).

```bash
pip install -r requirements.txt

export LLAMA_DIR=$PWD/../../../llama.cpp
```

### Generating the bindings: 4 options

Nothing is for free in like. [cffi] asks users to choose between [3 options](https://cffi.readthedocs.io/en/latest/cdef.html) to build and deploy modules. Well, we ask you to choose between the following *4* `MODE`s:

- `dynamic_load` (_Preferred for versioning_): just generate `*.py` and `*.pyi` files. Let cffi load `libllama.so` at execution time (it needs to be installed or its parent directory in your `LD_LIBRARY_PATH` or `DYLD_LIBRARY_PATH` env vars). This is the backup if the other options below fail, as it involves the least compilation. This is also a good mode to use to track generated files.

- `dynamic_link`: generate and build a native Python extension that dynamically links to `libllama.so`. Same as above, it needs to be installed / in ld path.

- `static_link` (_Preferred for local builds_): generate and build a native Python extension that statically links the compiled units from llama.cpp or ggml, so that it's self-sufficient at execution time. This is the preferred way if you're doing a local build.

- `inline` (__CPU support only__): have cffi build ggml.c by itself, which produces a self-sufficient Python extension that's unable to use METAL, CUDA or OPENCL acceleration :-S. But hey, that's the only mode that doesn't require you to build anything beforehand.

See [test_builds.sh](./test_builds.sh) for updated build instructions, or read on below.

#### Generate against llama.cpp

Prerequisite: build llama.cpp and ensure its shared library is in the ld path:

```bash
export LLAMA_DIR:-../../../llama.cpp

# Make sure to change -DLLAMA_METAL=1 for any other hardware specific flags, e.g. -DLLAMA_CUDA=1 or -DLLAMA_OPENCL=1:
( cmake "$LLAMA_DIR" -B llama_build \
    -DLLAMA_METAL=1 && \
    -DBUILD_SHARED_LIBS=1 && \
  cd llama_build && \
  make -j )

export LD_LIBRARY_PATH=$PWD/llama_build
export DYLD_LIBRARY_PATH=$PWD/llama_build
```

Then build, and run the example:

```bash
export LLAMA_BUILD_DIR=llama_build
rm ggml/cffi.*

# See the various mode options above
python generate.py --llama_cpp_dir=$LLAMA_DIR --build_dir=$LLAMA_BUILD_DIR --mode=static_link
```

### Point to libllama.so directly (no extension)

Alternatively you can load the compiled `libllama.so` binary w/ generated .py bindings:

```bash
# Note: use LLAMA_DEBUG=1 to debug any crashes
( cd $LLAMA_DIR && LLAMA_METAL=1 make clean libllama.so )

rm -fR ggml/cffi.*
COMPILE=0 python generate.py

# You can omit these if you've installed the library to say, /usr/lib
export DYLD_LIBRARY_PATH=$LLAMA_DIR:$DYLD_LIBRARY_PATH
export LD_LIBRARY_PATH=$LLAMA_DIR:$LD_LIBRARY_PATH

python example.py
```

### Alternatives

This simple example's primary goal is to showcase automatically generated & fast bindings.

- https://github.com/abetlen/ggml-python: these bindings seem to be hand-written and use [ctypes](https://docs.python.org/3/library/ctypes.html) (making its maintenance potentially error-prone and performance slower than compiled [cffi](https://cffi.readthedocs.io/) bindings). It has [high-quality API reference docs](https://ggml-python.readthedocs.io/en/latest/api-reference/#ggml.ggml), possibly making it a sounder choice for serious development.
  
- https://github.com/abetlen/llama-cpp-python: these expose the C++ `llama.cpp` interface, which this example cannot easily be extended to support (`cffi` only generates bindings of C libraries)

- [pybind11](https://github.com/pybind/pybind11) and [nanobind](https://github.com/wjakob/nanobind) are two alternatives to cffi that generate bindings for C++. Unfortunately none of them have an automatic generator so writing bindings is quite time-consuming.

### Caveats

While [cffi](https://cffi.readthedocs.io/) makes it trivial to keep up with any changes to the GGML API, it's using the pycparser package which seems a bit sensitive to exotic C syntaxes (the likes that can be found in Mac system headers, for instance), and it doesn't have its own C preprocessor. See [generate.py](./generate.py) to get a better idea of the what was needed to make this work.

### TODO

Generate .pyi file

```
cat ../../../llama.cpp/ggml*.h > ggml-all.h

import re
with open('ggml-all.h') as f: h = f.read()
re.sub(')

list(filter(lambda l: 'GGML_API' in l, re.sub(r'\s+', ' ', re.sub(r',\s*\n\s*', ', ', re.sub('//.*$', '', re.sub(r'/\*.*?\*/', '', h, re.M)), re.M)).splitlines()))
```